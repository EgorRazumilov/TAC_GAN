{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import tarfile\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_n_classes = []\n",
    "with open('./images_n_classes.txt') as inf:\n",
    "    for line in inf:\n",
    "        line = line.strip().split()\n",
    "        images_n_classes.append((line[0], line[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = []\n",
    "with open('./embs_skip_thoughts.txt') as inf:\n",
    "    for line in tqdm(inf):\n",
    "        line = line.split(';')[:-1]\n",
    "        line = [x.split() for x in line]\n",
    "        line = [[float(x) for x in y] for y in line]\n",
    "        embs.append(torch.tensor(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "            transforms.Resize(128),\n",
    "            transforms.CenterCrop(128),\n",
    "            transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_images = './jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgz = tarfile.open('./102flowers.tgz', 'r:gz')\n",
    "tgz.extractall()\n",
    "tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TAC_GAN_Dataset(Dataset):\n",
    "    def __init__(self, embs, images_classes, tra):\n",
    "        self.embs = embs\n",
    "        self.images_classes = images_n_classes\n",
    "    def __getitem__(self, index):\n",
    "        emb = self.embs[index]\n",
    "        image_name, class_folder = self.images_classes[index][0], self.images_classes[index][1]\n",
    "        \n",
    "        path = path_to_images + '/' + image_name\n",
    "        image = Image.open(path)\n",
    "        image = transform(image)\n",
    "        \n",
    "        max_rand = 5\n",
    "        text_i = random.randint(0, max_rand-1)\n",
    "        text = emb[text_i]\n",
    "        one_hot_classes = torch.zeros(102)\n",
    "        one_hot_classes[int(class_folder[6:]) - 1] = 1.0\n",
    "        return image, text, one_hot_classes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "dataset = TAC_GAN_Dataset(embs, images_n_classes)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_size = 100, embed_size = 4800, ner_fc1 = 256, ner_fc2 = 64, gen_conv_ch = 64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_shape = noise_size\n",
    "        self.embed_shape = embed_size\n",
    "        self.ner_fc1 = ner_fc1\n",
    "        self.ner_fc2 = ner_fc2\n",
    "\n",
    "        self.FC1 = nn.Linear(self.embed_shape, self.ner_fc1)\n",
    "        self.emb_leak = nn.LeakyReLU()\n",
    "        self.FC2 = nn.Linear(self.noise_shape + self.ner_fc1, 8*8*8*self.ner_fc2)\n",
    "        self.emb_bn = nn.BatchNorm2d(8*self.ner_fc2)\n",
    "        self.emb_rl = nn.ReLU()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8*self.ner_fc2, gen_conv_ch*4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(gen_conv_ch*4),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(gen_conv_ch*4, gen_conv_ch*2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(gen_conv_ch*2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(gen_conv_ch*2, gen_conv_ch, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(gen_conv_ch),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(gen_conv_ch, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.intialize_weights()\n",
    "\n",
    "    def forward(self, noise, embed):\n",
    "        batch_size = noise.shape[0]\n",
    "        latent_rep = self.emb_leak(self.FC1(embed))\n",
    "        x = torch.cat((noise, latent_rep), 1)\n",
    "        repr = self.FC2(x)\n",
    "        repr = self.emb_rl(self.emb_bn(repr.reshape((batch_size, 8*self.ner_fc2, 8, 8))))\n",
    "\n",
    "        img_f = self.net(repr)\n",
    "        return (img_f / 2.0) + 0.5\n",
    "    \n",
    "    def intialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                m.weight.data.normal_(0, 0.02)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embed_size = 4800, ner_fc1 = 256, out_net = 384, gen_conv_ch = 64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embed_shape = embed_size\n",
    "        self.ner_fc1 = ner_fc1\n",
    "        self.out_net = out_net\n",
    "\n",
    "        self.FC1 = nn.Linear(self.embed_shape, self.ner_fc1)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, self.out_net, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(self.out_net),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.conv_cat = nn.Conv2d(self.out_net + self.ner_fc1, 512, kernel_size=1, stride=1)\n",
    "        self.last_fc = nn.Linear(8*8*512, 64)\n",
    "        self.FC_real_fake = nn.Linear(64, 1)\n",
    "        self.FC_class = nn.Linear(64, 102)\n",
    "        self.leak = nn.LeakyReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.intialize_weights()\n",
    "        \n",
    "    def forward(self, img, emb):\n",
    "        batch_size = img.shape[0]\n",
    "        x = self.FC1(emb).reshape(batch_size, self.ner_fc1, -1).unsqueeze(2)\n",
    "        latent_repr = x.repeat(1, 1, 8, 8)\n",
    "\n",
    "        conved_img = self.net(img)\n",
    "\n",
    "        repr_cat = torch.cat((conved_img, latent_repr), 1)\n",
    "        to_fc = self.leak(self.conv_cat(repr_cat)).reshape((batch_size, 8*8*512))\n",
    "\n",
    "        to_fc = self.leak(self.last_fc(to_fc))\n",
    "\n",
    "        real_fake_dist = self.FC_real_fake(to_fc)\n",
    "        class_dist = self.sig(self.FC_class(to_fc))\n",
    "\n",
    "        return real_fake_dist, class_dist\n",
    "    \n",
    "    def intialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                m.weight.data.normal_(0, 0.02)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "criterion_class = nn.BCELoss()\n",
    "criterion_real_fake = nn.BCEWithLogitsLoss()\n",
    "\n",
    "netG = Generator()\n",
    "netG.to(device)\n",
    "\n",
    "netD = Discriminator()\n",
    "netD.to(device)\n",
    "\n",
    "optG = torch.optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optD = torch.optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "for i in range(N_EPOCHS):\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "    for i, (images, texts, labels) in enumerate(dataloader):\n",
    "        netG.train()\n",
    "        netD.train()\n",
    "        batch_size = images.shape[0]\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        real_label = torch.ones((batch_size, 1)).to(device)\n",
    "        fake_label = torch.zeros((batch_size, 1)).to(device)\n",
    "        #create noise for generator and normalize\n",
    "        noise = torch.randn(batch_size, 100)\n",
    "        noise.data.normal_(0,1)\n",
    "        noise = noise.to(device)\n",
    "        #create permutations for different case of learning\n",
    "        \n",
    "        rand_1 = torch.randperm(batch_size)\n",
    "        rand_1 = rand_1.to(device)\n",
    "        rand_2 = torch.randperm(batch_size)\n",
    "        rand_2 = rand_2.to(device)\n",
    "        rand_3 = torch.randperm(batch_size)\n",
    "        rand_3 = rand_3.to(device)\n",
    "        rand_4 = torch.randperm(batch_size)\n",
    "        rand_4 = rand_4.to(device)\n",
    "        \n",
    "        ############### Train D ###################\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        #train on real images, real classes, real captions\n",
    "        outS_real, outC_real, _ = netD(images, texts)\n",
    "        lossS_real = criterion_real_fake(outS_real, real_label)\n",
    "        lossC_real = criterion_class(outC_real, labels) \n",
    "        \n",
    "        #train on wrong images, wrong classes, real captions\n",
    "        outS_wrong, outC_wrong, _ = netD(images[rand_1], texts[rand_2])\n",
    "        lossS_wrong = criterion_real_fake(outS_wrong, fake_label)\n",
    "        lossC_wrong = criterion_class(outC_wrong, labels[rand_1])\n",
    "        \n",
    "        #train on fake images, real classes, real captions\n",
    "        fake_images = netG(noise, texts)\n",
    "        outS_fake, outC_fake, _ = netD(fake_images.detach(), texts[rand_3])\n",
    "        lossS_fake = criterion_real_fake(outS_fake, fake_label)\n",
    "        lossC_fake = criterion_class(outC_fake, labels[rand_3])\n",
    "        \n",
    "        #sum all losses\n",
    "        loss_D = (lossS_real + lossS_wrong + lossS_fake) + (lossC_real + lossC_wrong + lossC_fake)\n",
    "        loss_D.backward()\n",
    "        optD.step()\n",
    "        \n",
    "        ############### Train G ###################\n",
    "        netG.zero_grad()\n",
    "        noise.data.normal_(0,1)\n",
    "        fake_images = netG(noise, texts[rand_4])\n",
    "        S_fake, C_fake, _ = netD(fake_images, texts[rand_4])\n",
    "        lossS_G = criterion_real_fake(S_fake, real_label)\n",
    "        lossC_G = criterion_class(C_fake, labels[rand_4])\n",
    "        \n",
    "        loss_G = lossS_G + lossC_G\n",
    "        loss_G.backward()\n",
    "        optG.step()\n",
    "        ###########################################\n",
    "        if i % 5 == 0:\n",
    "            G_losses.append(loss_G.detach().cpu())\n",
    "            D_losses.append(loss_D.detach().cpu())\n",
    "            torch.save(netG.state_dict(), './netG.pt')\n",
    "            torch.save(netD.state_dict(), './netD.pt')\n",
    "            clear_output(True)\n",
    "            plt.figure(figsize=(10,5))\n",
    "            plt.title(\"Generator and DiscrimSinator Loss During Training\")\n",
    "            plt.plot(G_losses,label=\"G\")\n",
    "            plt.plot(D_losses,label=\"D\")\n",
    "            plt.xlabel(\"iterations\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            netG.eval()\n",
    "            with tf.device('cpu'):\n",
    "                with torch.no_grad():\n",
    "                    plt.imshow(fake_images[0].detach().cpu().permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
