{"cells":[{"metadata":{},"cell_type":"markdown","source":"For TPU"},{"metadata":{"trusted":true},"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nimport tarfile\n\nimport os\nimport shutil\nimport random\nfrom tqdm import tqdm\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images_n_classes = []\nwith open('../input/images-classes-for-oxford102/images_n_classes.txt') as inf:\n    for line in inf:\n        line = line.strip().split()\n        images_n_classes.append((line[0], line[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For TPU"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch_xla\nimport torch_xla.core.xla_model as xm\ndevice = xm.xla_device()\ntorch.set_default_tensor_type('torch.FloatTensor')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For USE"},{"metadata":{"trusted":true},"cell_type":"code","source":"embs = []\npath_to_classes = '../input/cvpr2016/text_c10/'\nwith tf.device('cpu'):\n    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n    model = hub.load(module_url)\n    for image_name, class_folder in images_n_classes:\n        text_name = image_name[:-4] + '.txt'\n        captions = []\n        with open(path_to_classes + class_folder + '/' + text_name)as inf:\n            for line in inf:\n                captions.append(line.strip())\n        emb = model(captions).numpy()\n        emb = torch.tensor(emb)\n        embs.append(emb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For SkipThougts"},{"metadata":{"trusted":true},"cell_type":"code","source":"embs = []\nwith open('../input/embs-for-skip/embs_skip_thoughts.txt') as inf:\n    for line in tqdm(inf):\n        line = line.split(';')[:-1]\n        line = [x.split() for x in line]\n        line = [[float(x) for x in y] for y in line]\n        embs.append(torch.tensor(line))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = transforms.Compose([\n            transforms.Resize(128),\n            transforms.CenterCrop(128),\n            transforms.ToTensor()\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path_to_images = './jpg'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tgz = tarfile.open('../input/flower-dataset-102/102flowers.tgz', 'r:gz')\ntgz.extractall()\ntgz.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TAC_GAN_Dataset(Dataset):\n    def __init__(self, embs, images_classes):\n        self.embs = embs\n        self.images_classes = images_n_classes\n    def __getitem__(self, index):\n        emb = self.embs[index]\n        image_name, class_folder = self.images_classes[index][0], self.images_classes[index][1]\n        \n        path = path_to_images + '/' + image_name\n        image = Image.open(path)\n        image = transform(image)\n        \n        max_rand = 5\n        text_i = random.randint(0, max_rand-1)\n        text = emb[text_i]\n        one_hot_classes = torch.zeros(102)\n        one_hot_classes[int(class_folder[6:]) - 1] = 1.0\n        return image, text, one_hot_classes\n    \n    def __len__(self):\n        return len(self.images_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"dataset = TAC_GAN_Dataset(embs, images_n_classes)\ndataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, noise_size = 100, embed_size = 4800, ner_fc1 = 256, ner_fc2 = 64, gen_conv_ch = 64):\n        super(Generator, self).__init__()\n        self.noise_shape = noise_size\n        self.embed_shape = embed_size\n        self.ner_fc1 = ner_fc1\n        self.ner_fc2 = ner_fc2\n\n        self.FC1 = nn.Linear(self.embed_shape, self.ner_fc1)\n        self.emb_leak = nn.LeakyReLU()\n        self.FC2 = nn.Linear(self.noise_shape + self.ner_fc1, 8*8*8*self.ner_fc2)\n        self.emb_bn = nn.BatchNorm2d(8*self.ner_fc2)\n        self.emb_rl = nn.ReLU()\n\n        self.net = nn.Sequential(\n            nn.ConvTranspose2d(8*self.ner_fc2, gen_conv_ch*4, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(gen_conv_ch*4),\n            nn.ReLU(),\n            nn.ConvTranspose2d(gen_conv_ch*4, gen_conv_ch*2, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(gen_conv_ch*2),\n            nn.ReLU(),\n            nn.ConvTranspose2d(gen_conv_ch*2, gen_conv_ch, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(gen_conv_ch),\n            nn.ReLU(),\n            nn.ConvTranspose2d(gen_conv_ch, 3, kernel_size=4, stride=2, padding=1),\n            nn.Tanh()\n        )\n        \n        self.intialize_weights()\n\n    def forward(self, noise, embed):\n        batch_size = noise.shape[0]\n        latent_rep = self.emb_leak(self.FC1(embed))\n        x = torch.cat((noise, latent_rep), 1)\n        repr = self.FC2(x)\n        repr = self.emb_rl(self.emb_bn(repr.reshape((batch_size, 8*self.ner_fc2, 8, 8))))\n\n        img_f = self.net(repr)\n        return (img_f / 2.0) + 0.5\n    \n    def intialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n                m.weight.data.normal_(0, 0.02)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, embed_size = 4800, ner_fc1 = 256, out_net = 384, gen_conv_ch = 64):\n        super(Discriminator, self).__init__()\n        self.embed_shape = embed_size\n        self.ner_fc1 = ner_fc1\n        self.out_net = out_net\n\n        self.FC1 = nn.Linear(self.embed_shape, self.ner_fc1)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(256, self.out_net, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(self.out_net),\n            nn.LeakyReLU(0.2)\n        )\n\n        self.conv_cat = nn.Conv2d(self.out_net + self.ner_fc1, 512, kernel_size=1, stride=1)\n        self.last_fc = nn.Linear(8*8*512, 64)\n        self.FC_real_fake = nn.Linear(64, 1)\n        self.FC_class = nn.Linear(64, 102)\n        self.leak = nn.LeakyReLU()\n        self.sig = nn.Sigmoid()\n        self.intialize_weights()\n        \n    def forward(self, img, emb):\n        batch_size = img.shape[0]\n        x = self.FC1(emb).reshape(batch_size, self.ner_fc1, -1).unsqueeze(2)\n        latent_repr = x.repeat(1, 1, 8, 8)\n\n        conved_img = self.net(img)\n\n        repr_cat = torch.cat((conved_img, latent_repr), 1)\n        to_fc = self.leak(self.conv_cat(repr_cat)).reshape((batch_size, 8*8*512))\n\n        to_fc = self.leak(self.last_fc(to_fc))\n\n        real_fake_dist = self.FC_real_fake(to_fc)\n        class_dist = self.sig(self.FC_class(to_fc))\n\n        return real_fake_dist, class_dist\n    \n    def intialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n                m.weight.data.normal_(0, 0.02)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda')\ncriterion_class = nn.BCELoss()\ncriterion_real_fake = nn.BCEWithLogitsLoss()\n\nnetG = Generator()\nnetG.to(device)\n#netG.apply(weights_init)\n\nnetD = Discriminator()\nnetD.to(device)\n#netD.apply(weights_init)\n\noptG = torch.optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptD = torch.optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_EPOCHS = 300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import clear_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"G_losses = []\nD_losses = []\nfor i in range(N_EPOCHS):\n    netG.train()\n    netD.train()\n    for i, (images, texts, labels) in enumerate(dataloader):\n        netG.train()\n        netD.train()\n        batch_size = images.shape[0]\n        images = images.to(device)\n        texts = texts.to(device)\n        labels = labels.to(device)\n        \n        real_label = torch.ones((batch_size, 1)).to(device)\n        fake_label = torch.zeros((batch_size, 1)).to(device)\n        #create noise for generator and normalize\n        noise = torch.randn(batch_size, 100)\n        noise.data.normal_(0,1)\n        noise = noise.to(device)\n        #create permutations for different case of learning\n        \n        rand_1 = torch.randperm(batch_size)\n        rand_1 = rand_1.to(device)\n        rand_2 = torch.randperm(batch_size)\n        rand_2 = rand_2.to(device)\n        rand_3 = torch.randperm(batch_size)\n        rand_3 = rand_3.to(device)\n        rand_4 = torch.randperm(batch_size)\n        rand_4 = rand_4.to(device)\n        \n        ############### Train D ###################\n        netD.zero_grad()\n        \n        #train on real images, real classes, real captions\n        outS_real, outC_real, _ = netD(images, texts)\n        lossS_real = criterion_real_fake(outS_real, real_label)\n        lossC_real = criterion_class(outC_real, labels) \n        \n        #train on wrong images, wrong classes, real captions\n        outS_wrong, outC_wrong, _ = netD(images[rand_1], texts[rand_2])\n        lossS_wrong = criterion_real_fake(outS_wrong, fake_label)\n        lossC_wrong = criterion_class(outC_wrong, labels[rand_1])\n        \n        #train on fake images, real classes, real captions\n        fake_images = netG(noise, texts)\n        outS_fake, outC_fake, _ = netD(fake_images.detach(), texts[rand_3])\n        lossS_fake = criterion_real_fake(outS_fake, fake_label)\n        lossC_fake = criterion_class(outC_fake, labels[rand_3])\n        \n        #sum all losses\n        loss_D = (lossS_real + lossS_wrong + lossS_fake) + (lossC_real + lossC_wrong + lossC_fake)\n        loss_D.backward()\n        optD.step()\n        \n        ############### Train G ###################\n        netG.zero_grad()\n        noise.data.normal_(0,1)\n        fake_images = netG(noise, texts[rand_4])\n        S_fake, C_fake, _ = netD(fake_images, texts[rand_4])\n        lossS_G = criterion_real_fake(S_fake, real_label)\n        lossC_G = criterion_class(C_fake, labels[rand_4])\n        \n        loss_G = lossS_G + lossC_G\n        loss_G.backward()\n        optG.step()\n        ###########################################\n        if i % 5 == 0:\n            G_losses.append(loss_G.detach().cpu())\n            D_losses.append(loss_D.detach().cpu())\n            torch.save(netG.state_dict(), './netG.pt')\n            torch.save(netD.state_dict(), './netD.pt')\n            clear_output(True)\n            plt.figure(figsize=(10,5))\n            plt.title(\"Generator and DiscrimSinator Loss During Training\")\n            plt.plot(G_losses,label=\"G\")\n            plt.plot(D_losses,label=\"D\")\n            plt.xlabel(\"iterations\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            plt.show()\n            netG.eval()\n            with tf.device('cpu'):\n                with torch.no_grad():\n                    plt.imshow(fake_images[0].detach().cpu().permute(1, 2, 0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If using USE"},{"metadata":{"trusted":true},"cell_type":"code","source":"netG.eval()\nwith tf.device('cpu'):\n    with torch.no_grad():\n        texts = [\"this flower is yellow in color, with petals that are very skinny\"]\n        emb = torch.tensor(model(text).numpy(), device=device)\n        emb = emb.view(1, -1)\n        images = netG(torch.randn(1, 100).to(device), emb)\n        images = images.squeeze().detach().cpu()\n        plt.imshow(images.permute(1, 2, 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}